\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\usepackage{graphicx}
\geometry{margin=1in}
\setlength\parindent{0pt}
\title{Foundations of Machine Learning -- Neural Networks}
\author{}
\date{}

\begin{document}
\maketitle


\section*{Neural Networks}

Lets revisit our regression optimization

\medskip

\textbf{Loss:}

\[
	\arg\min_{w} \frac{1}{N} \sum_{n=1}^{N} \left( w^T \phi(x^{(i)}) - y^{(i)} \right)^2
	= \arg\min_{w} \| \Phi w - y \|^2
\]

\[
	\text{Where } \Phi =
	\begin{bmatrix}
		\phi(x^{(1)}), \ldots, \phi(x^{(N)})
	\end{bmatrix}^T
	\in \mathbb{R}^{N \times M}, \quad
	w \in \mathbb{R}^M
\]

\textbf{Optimization:}

\begin{enumerate}
	\item Closed form solution:
	      \[
		      w^* = (\Phi^T \Phi)^{-1} \Phi^T y
	      \]

	\item Gradient descent:
	      \[
		      w^{(t)} = w^{(t-1)} - \epsilon \nabla_w \text{Loss}(w^{(t-1)})
	      \]
\end{enumerate}

Before, it was more or less a guess an check system to determine which bias function would be best to use for a given
dataset, but with neural networks we can learn the bias function during training!

\[
	\min_{w, \{\phi_j\}_{j=1}^M}
	\sum_i
	\left(
	\left(
		\sum_j w_j \phi_j(x^{(i)})
		\right)
	- y^{(i)}
	\right)^2
\]

The building block of neural networks is the perceptron

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{../imgs/perceptron.png} % 
\end{figure}

\pagebreak

\paragraph{Multilayer Perceptron (MLP)}
We can take this building block and create a system with multiple hidden layers.

\medskip

The Universal Approximation Theorem states that an MLP An MLP with varying complexity can approximate any continuous function to any desired degree of accuracy.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{../imgs/mlp-ex.png} % 
\end{figure}

\subsection*{Problems with MLP}
A 256x256 (RGB) image $\Rightarrow$ roughly 200k dimensional input x

A fully connected network would need a very large number of parameters, and very likely would overfit the data

Also generic deep networks also do not capture hte "natural" invariances we expect in images (translation, scale)
\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{../imgs/img-transofrmations.png}
\end{figure}

\pagebreak

\subsection*{Convolution}

MLPs treat every pixel as unique, so they must relearn the same feature (like a beak) at each position,
wasting parameters and computation. CNNs solve this by sharing weights through filters that slide across the image,
allowing one detector to recognize a pattern anywhere—making learning more efficient and translation-invariant.


\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{../imgs/convulution-ex.png}
\end{figure}

\subsection*{Convolution Operator}

A convolution filter is a small matrix of weights (for example, 3×3 or 5×5) that slides over the image.
Each position of the filter covers a small patch of the image and performs a dot product between its weights and the pixel values in that patch.

\medskip

Convolution filters detect local patterns such as edges, textures, and shapes.
They share weights across the image, reducing parameters and improving spatial efficiency. Through successive layers, they build hierarchical features, progressing from simple to complex representations. This structure allows the network to recognize features regardless of their position, providing translation invariance.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{../imgs/conv2.png}
\end{figure}

\pagebreak

\begin{figure}[h]
	\centering
	\includegraphics[width=0.55\textwidth]{../imgs/conv3.png}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.55\textwidth]{../imgs/conv4.png}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.55\textwidth]{../imgs/conv5.png}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.55\textwidth]{../imgs/conv6.png}
\end{figure}

\pagebreak

\subsection*{Max Pooling Operator}
Max pooling reduces the spatial size of feature maps while keeping key information.
It slides a small window (e.g. 2×2) over the input and outputs the maximum value within each window.
This keeps the strongest activations, discards weaker ones, and lowers computation.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{../imgs/pool1.jpg}
\end{figure}

\textbf{Full CNN in practice}

\begin{figure}[!ht]
	\centering
\includegraphics[width=0.7\textwidth]{../imgs/fullcnn.png}
\end{figure}



\subsection*{Convolution Operator}

A convolution filter is a small matrix of weights (for example, 3×3 or 5×5) that slides over the image.
Each position of the filter covers a small patch of the image and performs a dot product between its weights and the pixel values in that patch.

\medskip

Convolution filters detect local patterns such as edges, textures, and shapes.
They share weights across the image, reducing parameters and improving spatial efficiency. Through successive layers, they build hierarchical features, progressing from simple to complex representations. This structure allows the network to recognize features regardless of their position, providing translation invariance.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{../imgs/conv2.png}
\end{figure}

\pagebreak

\begin{figure}[h]
	\centering
	\includegraphics[width=0.55\textwidth]{../imgs/conv3.png}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.55\textwidth]{../imgs/conv4.png}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.55\textwidth]{../imgs/conv5.png}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.55\textwidth]{../imgs/conv6.png}
\end{figure}

\pagebreak

\subsection*{Max Pooling Operator}
Max pooling reduces the spatial size of feature maps while keeping key information.
It slides a small window (e.g. 2×2) over the input and outputs the maximum value within each window.
This keeps the strongest activations, discards weaker ones, and lowers computation.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{../imgs/pool1.jpg}
\end{figure}

\textbf{Full CNN in practice}

\begin{figure}[!ht]
	\centering
\includegraphics[width=0.7\textwidth]{../imgs/fullcnn.png}
\end{figure}



\section*{Recurrent Neural Networks}
 
The idea of RNNs is to allow us to have sequences of variable lengths and spatial/temporal correlations, 
such as sentences, videos, or time services that we want to analyze.

How we do this is by setting our hidden layers to have a recurrence as part of their input.


\paragraph{Elmans Network}
We do this through elman's networks which allows our hidden layers to have 
a recurrence as part of its input The ac1va1on value ht depends on $x_t$ but also $h_t-1$
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{../imgs/elman-net.png}  
\end{figure}

\pagebreak

If we look at the full network structure below we see this is a many-to-many relationship with many input nodes and many output nodes.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{../imgs/rnn-net.png}  
\end{figure}

In order to predict a future value we simply add another node to our output

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{../imgs/rnn-plus1.png}  
\end{figure}

\paragraph{Shared Parameters}
No matter how many times you unroll an RNN the number of weights and biasses will remain the same.
The parameters for the weights / biases are shared across the network.
\paragraph{Porblems With Basic RNNs}
One of the main problems with basic RNNs is the vanishing / exploding gradient problem. 

\medskip

Lets say our $w_h$ is set to 2, if we unroll our network lets say 50 times, then our input value becomes $= input \times 2^{50}$
This becomes a massive number and what this results in is that when we try to cacluate our gradient, because this number is so large our gradient also becomes very large leading to 
our gradient decent taking too big of steps.

\medskip

On the other side of the coin, lets say our $w_h$ is set to 0.5, if we unroll our network lets say 50 times, our input $= input \times 0.5^{50}$
Now we have a number very very close to 0 resulting in our gradient decent taking to small of steps and having difficulty finding the minimum.

\subsection*{Other Use Cases}
\textbf{Image Captioning}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{../imgs/rnn-image-cap.png}  
\end{figure}

\textbf{Sentiment Classification}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{../imgs/rnn-sen-clas.png}  
\end{figure}

\pagebreak

\section*{Graphs}

Graph examples
\begin{itemize}
    \item Social Networks
    \item Knowledge 
    \item Communication
    \item Neurons
\end{itemize}

There are three general types of predication tasks on graphs: 

\begin{itemize}
    \item graph-level: Goal is to predict the property of an entire graph
    \begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{../imgs/graph-level.png}  
\end{figure}
    \item node-level: Goal is to predict the identity or role of each node within a graph
    \begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{../imgs/node-level.png}  
\end{figure}
    \item edge-level: Goal is to predict the edge information (e.g. relationship between nodes)
    \begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{../imgs/edge-level.png}  
\end{figure}
\end{itemize}

\paragraph{Representation of Graphs}

Types of information that we will potentially want to use to make predictions:
\begin{itemize}
\item Nodes: node identity, number of neighbors
\item Edges: edge identity, connectivity
\item Global-context: number of nodes, longest path
\end{itemize}

\end{document}