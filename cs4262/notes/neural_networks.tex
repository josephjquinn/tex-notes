\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\usepackage{graphicx}
\geometry{margin=1in}
\setlength\parindent{0pt}
\title{Foundations of Machine Learning -- Neural Networks}
\author{}
\date{}

\begin{document}
\maketitle


\section*{Neural Networks}

Lets revisit our regression optimization

\medskip

\textbf{Loss:}

\[
	\arg\min_{w} \frac{1}{N} \sum_{n=1}^{N} \left( w^T \phi(x^{(i)}) - y^{(i)} \right)^2
	= \arg\min_{w} \| \Phi w - y \|^2
\]

\[
	\text{Where } \Phi =
	\begin{bmatrix}
		\phi(x^{(1)}), \ldots, \phi(x^{(N)})
	\end{bmatrix}^T
	\in \mathbb{R}^{N \times M}, \quad
	w \in \mathbb{R}^M
\]

\textbf{Optimization:}

\begin{enumerate}
	\item Closed form solution:
	      \[
		      w^* = (\Phi^T \Phi)^{-1} \Phi^T y
	      \]

	\item Gradient descent:
	      \[
		      w^{(t)} = w^{(t-1)} - \epsilon \nabla_w \text{Loss}(w^{(t-1)})
	      \]
\end{enumerate}

Before, it was more or less a guess an check system to determine which bias function would be best to use for a given
dataset, but with neural networks we can learn the bias function during training!

\[
	\min_{w, \{\phi_j\}_{j=1}^M}
	\sum_i
	\left(
	\left(
		\sum_j w_j \phi_j(x^{(i)})
		\right)
	- y^{(i)}
	\right)^2
\]

The building block of neural networks is the perceptron

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{../imgs/perceptron.png} % 
\end{figure}

\pagebreak

\paragraph{Multilayer Perceptron (MLP)}
We can take this building block and create a system with multiple hidden layers.

\medskip

The Universal Approximation Theorem states that an MLP An MLP with varying complexity can approximate any continuous function to any desired degree of accuracy.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{../imgs/mlp-ex.png} % 
\end{figure}

\subsection*{Problems with MLP}
A 256x256 (RGB) image $\Rightarrow$ roughly 200k dimensional input x

A fully connected network would need a very large number of parameters, and very likely would overfit the data

Also generic deep networks also do not capture hte "natural" invariances we expect in images (translation, scale)
\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{../imgs/img-transofrmations.png}
\end{figure}

\pagebreak

\subsection*{Convolution}

MLPs treat every pixel as unique, so they must relearn the same feature (like a beak) at each position,
wasting parameters and computation. CNNs solve this by sharing weights through filters that slide across the image,
allowing one detector to recognize a pattern anywhere—making learning more efficient and translation-invariant.


\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{../imgs/convulution-ex.png}
\end{figure}

\subsection*{Convolution Operator}

A convolution filter is a small matrix of weights (for example, 3×3 or 5×5) that slides over the image.
Each position of the filter covers a small patch of the image and performs a dot product between its weights and the pixel values in that patch.

\medskip

Convolution filters detect local patterns such as edges, textures, and shapes.
They share weights across the image, reducing parameters and improving spatial efficiency. Through successive layers, they build hierarchical features, progressing from simple to complex representations. This structure allows the network to recognize features regardless of their position, providing translation invariance.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{../imgs/conv2.png}
\end{figure}

\pagebreak

\begin{figure}[h]
	\centering
	\includegraphics[width=0.55\textwidth]{../imgs/conv3.png}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.55\textwidth]{../imgs/conv4.png}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.55\textwidth]{../imgs/conv5.png}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.55\textwidth]{../imgs/conv6.png}
\end{figure}

\pagebreak

\subsection*{Max Pooling Operator}
Max pooling reduces the spatial size of feature maps while keeping key information.
It slides a small window (e.g. 2×2) over the input and outputs the maximum value within each window.
This keeps the strongest activations, discards weaker ones, and lowers computation.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{../imgs/pool1.jpg}
\end{figure}

\textbf{Full CNN in practice}

\begin{figure}[!ht]
	\centering
\includegraphics[width=0.7\textwidth]{../imgs/fullcnn.png}
\end{figure}



\subsection*{Convolution Operator}

A convolution filter is a small matrix of weights (for example, 3×3 or 5×5) that slides over the image.
Each position of the filter covers a small patch of the image and performs a dot product between its weights and the pixel values in that patch.

\medskip

Convolution filters detect local patterns such as edges, textures, and shapes.
They share weights across the image, reducing parameters and improving spatial efficiency. Through successive layers, they build hierarchical features, progressing from simple to complex representations. This structure allows the network to recognize features regardless of their position, providing translation invariance.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{../imgs/conv2.png}
\end{figure}

\pagebreak

\begin{figure}[h]
	\centering
	\includegraphics[width=0.55\textwidth]{../imgs/conv3.png}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.55\textwidth]{../imgs/conv4.png}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.55\textwidth]{../imgs/conv5.png}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.55\textwidth]{../imgs/conv6.png}
\end{figure}

\pagebreak

\subsection*{Max Pooling Operator}
Max pooling reduces the spatial size of feature maps while keeping key information.
It slides a small window (e.g. 2×2) over the input and outputs the maximum value within each window.
This keeps the strongest activations, discards weaker ones, and lowers computation.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{../imgs/pool1.jpg}
\end{figure}

\textbf{Full CNN in practice}

\begin{figure}[!ht]
	\centering
\includegraphics[width=0.7\textwidth]{../imgs/fullcnn.png}
\end{figure}



\section*{Recurrent Neural Networks}
 
The idea of RNNs is to allow us to have sequences of variable lengths and spatial/temporal correlations, 
such as sentences, videos, or time services that we want to analyze.

How we do this is by setting our hidden layers to have a recurrence as part of their input.


\paragraph{Elmans Network}
We do this through elman's networks which allows our hidden layers to have 
a recurrence as part of its input The ac1va1on value ht depends on $x_t$ but also $h_t-1$
\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.9\textwidth]{../imgs/elman-net.png}  
\end{figure}

\pagebreak

If we look at the full network structure below we see this is a many-to-many relationship with many input nodes and many output nodes.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{../imgs/rnn-net.png}  
\end{figure}

In order to predict a future value we simply add another node to our output

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{../imgs/rnn-plus1.png}  
\end{figure}

\paragraph{Shared Parameters}
No matter how many times you unroll an RNN the number of weights and biasses will remain the same.
The parameters for the weights / biases are shared across the network.
\paragraph{Porblems With Basic RNNs}
One of the main problems with basic RNNs is the vanishing / exploding gradient problem. 

\medskip

Lets say our $w_h$ is set to 2, if we unroll our network lets say 50 times, then our input value becomes $= input \times 2^{50}$
This becomes a massive number and what this results in is that when we try to cacluate our gradient, because this number is so large our gradient also becomes very large leading to 
our gradient decent taking too big of steps.

\medskip

On the other side of the coin, lets say our $w_h$ is set to 0.5, if we unroll our network lets say 50 times, our input $= input \times 0.5^{50}$
Now we have a number very very close to 0 resulting in our gradient decent taking to small of steps and having difficulty finding the minimum.

\subsection*{Other Use Cases}
\textbf{Image Captioning}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{../imgs/rnn-image-cap.png}  
\end{figure}

\textbf{Sentiment Classification}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{../imgs/rnn-sen-clas.png}  
\end{figure}

\pagebreak

\section*{Graphs}

Graph examples
\begin{itemize}
    \item Social Networks
    \item Knowledge 
    \item Communication
    \item Neurons
\end{itemize}

There are three general types of predication tasks on graphs: 

\begin{itemize}
    \item graph-level: Goal is to predict the property of an entire graph
    \begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{../imgs/graph-level.png}  
\end{figure}
    \item node-level: Goal is to predict the identity or role of each node within a graph
    \begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{../imgs/node-level.png}  
\end{figure}
    \item edge-level: Goal is to predict the edge information (e.g. relationship between nodes)
    \begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{../imgs/edge-level.png}  
\end{figure}
\end{itemize}

\paragraph{Representation of Graphs}

Types of information that we will potentially want to use to make predictions:
\begin{itemize}
\item Nodes: node identity, number of neighbors
\item Edges: edge identity, connectivity
\item Global-context: number of nodes, longest path
\end{itemize}

The reason we don't just use adjacency matrix as input vector to a neural network is because it is not permutation invariant, and is space-inefficient (very sparse adjacency matrices).

\paragraph*{GNN}
This involves a separate multilayer perceptron (MLP) on each component of a graph
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{../imgs/gnn-net.png}  
\end{figure}

\paragraph*{GNN Pooling}
This is a way to collect information from edges and give them to nodes for prediction.
\medskip

For each item to be pooled, gather each of their embeddings and concatenate them into a matrix. The gathered embeddings are then aggregated, usually via a sum operation.

\pagebreak

\section*{Deep Sets}

With sets, we have a group of data where order does not matter, when working with standard MLPS on this input data we run into the issue 
where different permutations of hte data lead to different  results:

\begin{figure}[h]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../imgs/set-ex.png}
        \label{fig:set-ex}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../imgs/set-permutations.png}
        \label{fig:set-permutations}
    \end{minipage}
\end{figure}

One option that we have is for each permutation of our input data

\begin{enumerate}
  \item \textbf{Generate all permutations:} \\
  Since the order of elements shouldn’t matter, you can imagine training the neural network to produce the same output for all possible input orders.
  \begin{itemize}
    \item For \( N = 5 \), that’s \( 5! = 120 \) permutations — quickly intractable as \( N \) grows.
  \end{itemize}

  \item \textbf{Neural network \(\psi\):} \\
  Each permutation of the set is fed into a neural network \(\psi\), producing embeddings \( h_1, h_2, h_3, \ldots \)

  \item \textbf{Aggregate function \(f\):} \\
  These embeddings are combined using a \textbf{permutation-invariant} aggregation (e.g., sum, mean, max).

  \item \textbf{Final mapping \(\phi\):} \\
  The aggregated representation is passed through another neural network \(\phi\) to produce the final output \( y \).
\end{enumerate}

But there is a problem, generating all $N!$ permutations is computationally infeasible. To reduce cost, one could use only k-tuples (subsets of the full set) where 
$k<N$, but that only partially captures the full combinatorial space.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{../imgs/permute-nn-set.png}  
\end{figure}

\[
 P(N,k) = \frac{N!}{(N-k)!} = \text{\# of permutations of N elements taken k at a time}
\]

Having a higher k captures more relationships (higher-order dependencies) between elements.
whereas a smaller k is cheaper but models only limited interactions.

\paragraph*{Deep Set} When $k=1$, this is a special case called a deep set.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.55\textwidth]{../imgs/deep-set.png}  
\end{figure}

\section*{Self Supervised Learning}
The idea here is that this is an technique where a model learns from unlabeled data by generating its own training signals.

\medskip

Instead of relying on human-annotated labels, the model creates pseudo-labels or pretext tasks from raw data. By solving these tasks, 
it learns useful internal representations that can later be fine-tuned for real downstream tasks.

\medskip

The reason for this existing is the cost of good quality labelled data is very high in terms of time and money

\pagebreak
\paragraph*{Process}
\begin{enumerate}
    \item \textbf{Pretext task} --- The model predicts some part of the data from other parts. \\
    Examples:
    \begin{itemize}
        \item Predict the missing word in a sentence (language models).
        \item Predict the rotation angle of an image (vision models).
        \item Reconstruct masked pixels or audio segments.
    \end{itemize}

    \item \textbf{Representation learning} --- By solving the pretext task, the model learns general features of the data (like patterns, structures, relationships).

    \item \textbf{Fine-tuning} --- These learned representations are transferred to a smaller labeled dataset for a supervised task.
\end{enumerate}

\paragraph*{Examples}
\begin{itemize}
	\item Pretext task: train a model to predict the rotation degree of rotated images with
cats and dogs (we can collect million of images from internet, labeling is not
required)

\begin{figure}[h]
	\centering
	\includegraphics[width=0.55\textwidth]{../imgs/ssl-cat2.png}  
\end{figure}


\item Downstream task: use transfer learning and fine-tune the learned model from the
pretext task for classification of cats vs dogs with very few labeled examples

\begin{figure}[h]
	\centering
	\includegraphics[width=0.55\textwidth]{../imgs/ssl-cat1.png}  
\end{figure}
\end{itemize}

Another example of a pretext task:
Predict missing pieces (context encoders or inpainting)
\begin{itemize}
	\item Training data: remove a random region in images
\item Pretext task: fill in a missing piece in the image
The model needs to understand the content of the entire image, and produce a
plausible replacement for the missing piece

\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{../imgs/ssl-img1.png}  
\end{figure}


\end{document}