\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\geometry{margin=1in}
\title{Foundations of Machine Learning -- Lecture 4 Notes}
\author{}
\date{}

\begin{document}
\maketitle

\section*{Nonlinear Regression}

So far, we have been using a linear function for regression:
\[
	f(x) = w^T x + w_0 = \sum_{i=0}^d w_i \hat{x}_i \quad \text{(Assuming $\hat{x}_0 = 1$)}
\]

Let’s generalize this model:

\[
	f(x) = \sum_{i=0}^M w_i \phi_i(x) = w^T \phi(x)
\]

where $\phi_i$ are fixed ``basis'' functions.

\medskip

Now with loss function:

\[
	\arg\min_{w} \frac{1}{N} \sum_{n=1}^N \left( w^T \phi(x^{(i)}) - y^{(i)} \right)^2
	= \arg\min_{w} \| \Phi w - y \|^2
\]

Where

\[
	\Phi = \begin{bmatrix}
		\phi(x^{(1)}), \ldots, \phi(x^{(N)})
	\end{bmatrix}^T \in \mathbb{R}^{N \times M},
	\quad w \in \mathbb{R}^M.
\]

Now we solve:

\[
	w^* = (\Phi^T \Phi)^{-1} \Phi^T y
\]

\paragraph*{Radial Basis Function (RBF) Regression}

\[
	\phi_j(x) = \exp\!\left(-\frac{(x - \mu_j)^2}{2s^2}\right)
\]

\paragraph*{Sigmoidal Basis Regression}
\[
	\phi_j(x) = \text{sigmoid}\!\left(\frac{x - \mu_j}{s}\right)
\]

\begin{itemize}
	\item \textbf{Pros:} They lead to convex optimization!, Allow for incorporating prior knowledge about your data.
	\item \textbf{Cons:} Are sensitive to the choice of parameters and are most
	      certainly sub-optimal, Severely suffer from curse of dimensionality and parameter
	      explosion
\end{itemize}

\paragraph{Explosion of terms}
Let’s look at the number of terms for a polynomial of degree 2 in different dimensions, $d$:
\begin{itemize}
	\item $d = 1$: \quad $\phi(x) = [1, x, x^2]^T$
	\item $d = 2$: \quad $\phi(x) = [1, [x]_1, [x]_2, [x]_1[x]_2, [x]_1^2, [x]_2^2]^T$
	\item $d = 3$: \quad $\phi(x) = [1, [x]_1, [x]_2, [x]_3, [x]_1[x]_2, [x]_1[x]_3, [x]_2[x]_3, [x]_1^2, [x]_2^2, [x]_3^2]^T$
\end{itemize}

\vspace{0.4cm}
In general, a polynomial of degree $K$, in a $d$-dimensional space will have:
\[
	\text{Num. terms} = \binom{K+d}{K} = \frac{(K+d)!}{K! \, d!} \sim O(d^K)
\]

\vspace{0.4cm}
For MNIST (a small-scale dataset!), $d = 784$, therefore:
\begin{itemize}
	\item Polynomial of degree $K=2$, will have $307{,}729$ terms
	\item Polynomial of degree $K=3$, will have $80{,}622{,}640$ terms
\end{itemize}


\subsection*{Learnable Feature Maps}

What if we optimize the feature maps?
This means that the features are not fixed and learn with the parameters.
\[
	f\!\left(x;\{w_i,\mu_i,\sigma_i\}_{i=1}^M\right)
	= \sum_{i=1}^{M} w_i\,\phi\!\left(x;\mu_i,\sigma_i\right)
\]

\vspace{0.75em}
\textbf{Learning problem:}
\[
	\arg\min_{\{w_i,\mu_i,\sigma_i\}_{i=1}^M}
	\sum_{n=1}^{N}\!\left(
	\sum_{i=1}^{M} w_i\,\phi\!\left(x_n;\mu_i,\sigma_i\right) - y_n
	\right)^{\!2}
\]

\vspace{0.75em}
\textit{Deep learning: features are learned from data.}

\subsection*{Kernel Methods}
The idea behind kernel methods is to map the data into a higher-dimensional space, perform regression in that space, and then transform the results back to the original space. This allows complex decision surfaces to be represented as linear operations in the transformed space.

\subsection*{Robust Regression}
How can we ignore some outliers?

\[
	\mathcal{L}(w) = \sum_{i=1}^N \bigl\lVert y^{(i)} - f(x^{(i)}) \bigr\rVert^2
	\quad \Longrightarrow \quad
	\mathcal{L}(w) = \sum_{i=1}^N \alpha^{(i)} \bigl\lVert y^{(i)} - f(x^{(i)}) \bigr\rVert^2
\]

If we can set $\alpha^{(i)}$ of outliers to zero or close to zero, we can ignore them and they wont affect our model.

We can also have adaptive weights where weights are defined as a function of the residual:
\[
	\alpha^{(i)} = h(\epsilon^{(i)}).
\]

A common choice is:
\[
	h(\epsilon) = \frac{1}{\max(\delta, \epsilon)},
\]

Meaning: the weight of each point is reduced if its error is large. Where $\delta > 0$ prevents division by zero.

\paragraph{Effect}
\begin{itemize}
	\item For small residuals ($\epsilon$ small), $\alpha^{(i)}$ is close to 1, so the point keeps full influence.
	\item For large residuals (outliers), $\alpha^{(i)}$ becomes small, reducing its effect.
\end{itemize}

\end{document}

